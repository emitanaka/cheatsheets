[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a collection of my cheatsheets that I’m sharing in case it’s also useful to someone else other than me. If there are any corrections needed, please let me know."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Emi’s Cheatsheets",
    "section": "",
    "text": "This is a collection of my cheatsheets that I’m sharing in case it’s also useful to someone else other than me. If there are any corrections needed, please let me know."
  },
  {
    "objectID": "linear-algebra.html",
    "href": "linear-algebra.html",
    "title": "Linear Algebra",
    "section": "",
    "text": "The following conventions are used unless stated otherwise:\n\nAn italic, non-bold lower-case letter is a scalar, e.g. \\(a, b\\).\nAn italic, bold lower-case letter is a vector, e.g. \\(\\boldsymbol{v}, \\boldsymbol{w}\\).\nA non-italic, bold upper-case letter is a matrix, e.g. \\(\\mathbf{A}, \\mathbf{B}\\).\n\nMatrix is a rectangular (two-dimensional) array of numbers arranged in rows and columns, with individual entries in the array referred to as elements or terms of the matrix.\nA matrix \\(\\mathbf{A}\\) with 2 rows and 3 columns can be conveniently written as \\(\\mathbf{A}_{2\\times 3}\\) with dimensions on the subscript, or \\[\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23}\n\\end{bmatrix} = \\{a_{ij}\\}\\quad\\text{for}\\quad i=1,2;~j=1,2,3.\\]\nA vector is a column vector (i.e. a matrix with a single column) in this cheatsheet.\nThe elements of a matrix are usually scalar. A scalar can be thought of as a matrix with order \\(1\\times 1\\).\n\n\n\n\n\\(\\text{diag}\\left(\\mathbf{A}_{r\\times r}\\right) = (a_{11}, a_{22}, ..., a_{rr})^{\\scriptscriptstyle \\top}\\)\n\\(\\text{diag}\\left(a_{11}, a_{22}, ..., a_{rr}\\right) = \\begin{bmatrix}a_{11} & 0 & \\cdots & 0\\\\ 0 & a_{22} & \\ddots & \\vdots \\\\ \\vdots & \\ddots & \\ddots & 0 \\\\ 0 & \\cdots & 0 & a_{rr} \\end{bmatrix}\\)\nIf \\(\\boldsymbol{a}=(a_{11}, a_{22}, ..., a_{rr})^{\\scriptscriptstyle \\top}\\), then \\[\\text{diag}(\\boldsymbol{a})=\\text{diag}\\left(a_{11}, a_{22}, ..., a_{rr}\\right).\\]\n\\({\\rm rank}\\left(\\mathbf{A}\\right)\\) is the rank of a matrix \\(\\mathbf{A}\\).\n\\({\\rm tr}\\left(\\mathbf{A}\\right)\\) is the trace of a matrix \\(\\mathbf{A}\\).\n\\(\\mathcal{R}(\\mathbf{A})\\) is the range/span/column space of a matrix \\(\\mathbf{A}\\).\n\\(\\mathcal{N}(\\mathbf{A})\\) is the kernel/null space of a matrix \\(\\mathbf{A}\\)."
  },
  {
    "objectID": "linear-algebra.html#traces",
    "href": "linear-algebra.html#traces",
    "title": "Linear Algebra",
    "section": "Traces",
    "text": "Traces\nThe trace of a square \\(n\\times n\\) matrix \\(\\mathbf{A}\\) is the sum of the diagonal elements and denoted \\({\\rm tr}\\left(\\mathbf{A}\\right)\\). The trace is not defined for a matrix that is not a square.\nLet \\(\\lambda_1, \\lambda_2, ..., \\lambda_n\\) be the eigenvalues of \\(\\mathbf{A}\\); \\(\\mathbf{B}\\) and \\(\\mathbf{C}\\) are \\(n\\times n\\) matrices; and \\(\\boldsymbol{x}\\) and \\(\\boldsymbol{y}\\) are vector of length \\(n\\).\n\n\\({\\rm tr}\\left(\\mathbf{A}\\right) = {\\rm tr}\\left(\\mathbf{A}^{\\scriptscriptstyle \\top}\\right)\\)\nIf \\(a\\) is a scalar, then \\({\\rm tr}\\left(a\\right)=a\\).\n\\({\\rm tr}\\left(\\mathbf{A}+ \\mathbf{B}\\right) = {\\rm tr}\\left(\\mathbf{A}\\right) + {\\rm tr}\\left(\\mathbf{B}\\right)\\)\n\\({\\rm tr}\\left(\\mathbf{A}\\mathbf{B}\\right) = {\\rm tr}\\left(\\mathbf{B}\\mathbf{A}\\right) = \\sum_{i=1}^r\\sum_{j=1}^c a_{ij}b_{ji}\\)\n\\({\\rm tr}\\left(\\mathbf{A}\\mathbf{A}^{\\scriptscriptstyle \\top}\\right) = {\\rm tr}\\left(\\mathbf{A}^{\\scriptscriptstyle \\top}\\mathbf{A}\\right) = \\sum_{i=1}^r\\sum_{j=1}^c a_{ij}^2\\)\n\\({\\rm tr}\\left(\\mathbf{A}\\mathbf{B}\\mathbf{C}\\right) = {\\rm tr}\\left(\\mathbf{B}\\mathbf{C}\\mathbf{A}\\right) = {\\rm tr}\\left(\\mathbf{C}\\mathbf{A}\\mathbf{B}\\right)\\). Note these are note equal to \\({\\rm tr}\\left(\\mathbf{C}\\mathbf{B}\\mathbf{A}\\right)\\).\n\\(\\boldsymbol{x}^{\\scriptscriptstyle \\top}\\mathbf{A}\\boldsymbol{y}= {\\rm tr}\\left(\\boldsymbol{x}^{\\scriptscriptstyle \\top}\\mathbf{A}\\boldsymbol{y}\\right) = {\\rm tr}\\left(\\mathbf{A}\\boldsymbol{y}\\boldsymbol{x}^{\\scriptscriptstyle \\top}\\right)\\)\n\\({\\rm tr}\\left(\\mathbf{A}\\right) = \\sum_i^n \\lambda_i\\)\n\\({\\rm tr}\\left(\\mathbf{A}^k\\right) = \\sum_i^n \\lambda_i^k\\)"
  },
  {
    "objectID": "linear-algebra.html#transpose",
    "href": "linear-algebra.html#transpose",
    "title": "Linear Algebra",
    "section": "Transpose",
    "text": "Transpose\n\nIf \\(a_{ij}\\) is the \\((i,j)\\)-th element of \\(\\mathbf{A}\\) then \\(a_{ij}\\) is the \\((j,i)\\)-th element of \\(\\mathbf{A}^\\top\\).\nThe transpose operation is reflexive: \\((\\mathbf{A}^{\\scriptscriptstyle \\top})^{\\scriptscriptstyle \\top}= \\mathbf{A}\\).\n\\((\\mathbf{A}+ \\mathbf{B})^{\\scriptscriptstyle \\top}= \\mathbf{A}^{\\scriptscriptstyle \\top}+ \\mathbf{B}^{\\scriptscriptstyle \\top}\\)\n\\((\\mathbf{A}\\mathbf{B})^{\\scriptscriptstyle \\top}= \\mathbf{B}^{\\scriptscriptstyle \\top}\\mathbf{A}^{\\scriptscriptstyle \\top}\\)\nTransposing a partitioned matrix: \\[\\begin{bmatrix}\n      \\mathbf{A}& \\mathbf{B}& \\mathbf{C}\\\\    \n      \\mathbf{D} & \\mathbf{E} & \\mathbf{F}\n      \\end{bmatrix}^{\\scriptscriptstyle \\top}= \\begin{bmatrix}\n      \\mathbf{A}^{\\scriptscriptstyle \\top}& \\mathbf{D}^{\\scriptscriptstyle \\top}\\\\\n      \\mathbf{B}^{\\scriptscriptstyle \\top}& \\mathbf{E}^{\\scriptscriptstyle \\top}\\\\\n      \\mathbf{C}^{\\scriptscriptstyle \\top}& \\mathbf{F}^{\\scriptscriptstyle \\top}\n      \\end{bmatrix}\\]\n\\(\\left(\\mathbf{A}^{-1}\\right)^{\\scriptscriptstyle \\top}= \\left(\\mathbf{A}^{\\scriptscriptstyle \\top}\\right)^{-1}\\)"
  },
  {
    "objectID": "linear-algebra.html#inverses",
    "href": "linear-algebra.html#inverses",
    "title": "Linear Algebra",
    "section": "Inverses",
    "text": "Inverses\n\nThe inverse of a square matrix \\(\\mathbf{A}\\) is the unique matrix \\(\\mathbf{A}^{-1}\\) such that \\(\\mathbf{A}^{-1}\\mathbf{A}=\\mathbf{A}\\mathbf{A}^{-1}=\\mathbf{I}.\\) Note that \\(\\mathbf{A}^{-1}\\) does not always exist.\n\\(\\mathbf{A}^{-1} = |\\mathbf{A}|^{-1}\\text{adj}\\mathbf{A}\\) where \\(\\text{adj}\\mathbf{A}\\) is the adjugate (or adjoint) matrix of \\(\\mathbf{A}\\) (the transpose of the matrix of co-factors).\nInverse of a matrix \\(\\mathbf{A}\\) does not exist if \\(|\\mathbf{A}| = 0\\) and \\(\\mathbf{A}\\) is said to be singular.\n\\((\\mathbf{A}^{\\scriptscriptstyle \\top})^{-1} = (\\mathbf{A}^{-1})^{\\scriptscriptstyle \\top}\\).\nIf \\(\\mathbf{A}\\) is symmetric so is its inverse: if \\(\\mathbf{A}^{\\scriptscriptstyle \\top}= \\mathbf{A}\\) then \\((\\mathbf{A}^{-1})^{\\scriptscriptstyle \\top}= \\mathbf{A}^{-1}\\).\n\\((\\mathbf{A}\\otimes \\mathbf{B})^{-1} = \\mathbf{A}^{-1}\\otimes \\mathbf{B}^{-1}\\)\n\\((\\mathbf{A}\\oplus \\mathbf{B})^{-1} = \\mathbf{A}^{-1}\\oplus \\mathbf{B}^{-1}\\)\n\\((\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}\\).\n\\((a\\mathbf{I}_n + b\\mathbf{J}_n)^{-1} = \\dfrac{1}{a}\\left(\\mathbf{I}_n - \\dfrac{b}{a+nb}\\mathbf{J}_n\\right)\\) if \\(a\\neq =0\\) and \\(a + nb \\neq 0\\).\n\\[\\begin{bmatrix}\n      \\mathbf{R} & \\boldsymbol{0} \\\\\n      \\mathbf{X}& \\mathbf{S}\n      \\end{bmatrix}^{-1} = \\begin{bmatrix}\n      \\mathbf{R}^{-1} & \\boldsymbol{0} \\\\\n      -\\mathbf{S}^{-1}\\mathbf{X}\\mathbf{R}^{-1} & \\mathbf{S}^{-1}\n      \\end{bmatrix}\\]\n\\(\\displaystyle \\mathbf{A}= \\begin{bmatrix}  a & b \\\\  c & d  \\end{bmatrix}\\) then \\(\\mathbf{A}^{-1} = \\dfrac{1}{ab - cd} \\begin{bmatrix}  d & -b \\\\  -c & a  \\end{bmatrix}\\).\nSuppose we have the partitioned matrix \\(\\mathbf{A}= \\begin{bmatrix}  \\mathbf{A}_{11} & \\mathbf{A}_{12} \\\\  \\mathbf{A}_{21} & \\mathbf{A}_{22}  \\end{bmatrix}\\) then \\(\\mathbf{A}^{-1}\\) \\[\\begin{bmatrix}\n      (\\mathbf{A}_{11} - \\mathbf{A}_{12}\\mathbf{A}_{22}^{-1}\\mathbf{A}_{21})^{-1} & -\\mathbf{A}_{11}^{-1}\\mathbf{A}_{12}(\\mathbf{A}_{22} - \\mathbf{A}_{21}\\mathbf{A}_{11}^{-1}\\mathbf{A}_{12})^{-1} \\\\\n      -\\mathbf{A}_{22}^{-1}\\mathbf{A}_{21}(\\mathbf{A}_{11} - \\mathbf{A}_{12}\\mathbf{A}_{22}^{-1}\\mathbf{A}_{21})^{-1} & (\\mathbf{A}_{22} - \\mathbf{A}_{21}\\mathbf{A}_{11}^{-1}\\mathbf{A}_{12})^{-1}\n      \\end{bmatrix}\\] \\[\\begin{bmatrix}\n      (\\mathbf{A}_{11} - \\mathbf{A}_{12}\\mathbf{A}_{22}^{-1}\\mathbf{A}_{21})^{-1} & -(\\mathbf{A}_{11} - \\mathbf{A}_{12}\\mathbf{A}_{22}^{-1}\\mathbf{A}_{21})^{-1}\\mathbf{A}_{12}\\mathbf{A}_{22}^{-1} \\\\\n      -(\\mathbf{A}_{22} - \\mathbf{A}_{21}\\mathbf{A}_{11}^{-1}\\mathbf{A}_{12})^{-1}\\mathbf{A}_{21}\\mathbf{A}_{11}^{-1} & (\\mathbf{A}_{22} - \\mathbf{A}_{21}\\mathbf{A}_{11}^{-1}\\mathbf{A}_{12})^{-1}\n      \\end{bmatrix}\\]\n\n\nWoodbury matrix identity\n\\[\\left(\\mathbf{A}+ \\mathbf{U}\\mathbf{B}\\mathbf{V}\\right)^{-1} = \\mathbf{A}^{-1} - \\mathbf{A}^{-1}\\mathbf{U}(\\mathbf{B}^{-1} + \\mathbf{V}\\mathbf{A}^{-1}\\mathbf{U})^{-1}\\mathbf{V}\\mathbf{A}^{-1}.\\] Alternatively, deal with \\[\\left(\\mathbf{A}+ \\mathbf{U}\\mathbf{B}\\mathbf{V}\\right)^{-1} = \\mathbf{A}^{-1} - \\mathbf{A}^{-1}\\mathbf{U}\\mathbf{B}\\boldsymbol{V}(\\mathbf{I} + \\mathbf{A}^{-1}\\mathbf{U}\\mathbf{B}\\mathbf{V})^{-1}\\mathbf{A}^{-1}.\\] Special case: \\[(\\mathbf{A}+ \\boldsymbol{u}\\boldsymbol{v}^{\\scriptscriptstyle \\top})^{-1} = \\mathbf{A}^{-1} - \\dfrac{\\mathbf{A}^{-1}\\boldsymbol{u}\\boldsymbol{v}^{\\scriptscriptstyle \\top}\\mathbf{A}^{-1}}{1 + \\boldsymbol{v}^{\\scriptscriptstyle \\top}\\mathbf{A}^{-1}\\boldsymbol{u}}\\]\n\n\nGeneralised inverse\n\nFor a given matrix \\(\\mathbf{A}\\in \\mathbb{R}^{n \\times m}\\), if \\(\\mathbf{A}^{-} \\in \\mathbb{R}^{m \\times n}\\) is such that \\[\\boldsymbol{AA}^{-}\\mathbf{A}= \\mathbf{A}\\] then \\(\\mathbf{A}^{-}\\) is a generalised inverse of \\(\\mathbf{A}\\). If \\(\\mathbf{A}^{-}\\) also satisfies \\[\\mathbf{A}^{-}\\mathbf{A}\\mathbf{A}^{-} = \\mathbf{A}^{-}\\] then \\(\\mathbf{A}^{-}\\) is a generalised reflexive inverse of .\n\nIf \\(\\mathbf{A}^{-}\\) satisfies the above two conditions and also \\[ (\\mathbf{A}\\mathbf{A}^{-})^{\\scriptscriptstyle \\top}= \\mathbf{A}\\mathbf{A}^{-}\\quad\\text{ and }\\quad (\\mathbf{A}^{-}\\mathbf{A})^{\\scriptscriptstyle \\top}= \\mathbf{A}^{-}\\mathbf{A},\\] then \\(\\mathbf{A}^{-}\\) is the Moore-Penrose pseudoinverse of \\(\\mathbf{A}\\).\n\n\\(\\mathbf{A}^-\\) is generally not unique (as opposed to \\(\\mathbf{A}^{-1}\\)) although the Moore-Penrose pseudoinverse exists and unique for any matrix.\nFor symmetric matrix \\(\\mathbf{A}\\), \\(\\mathbf{G}=\\mathbf{A}^-\\) may not be symmetric although \\(\\mathbf{G}^{\\scriptscriptstyle \\top}\\) is still a generalised inverse."
  },
  {
    "objectID": "linear-algebra.html#derivatives",
    "href": "linear-algebra.html#derivatives",
    "title": "Linear Algebra",
    "section": "Derivatives",
    "text": "Derivatives\nLet \\(a\\) be a constant that is not a function of \\(\\boldsymbol{x}\\); \\(b\\) a constant that is a function of \\(\\boldsymbol{x}\\); vectors \\(\\boldsymbol{u}\\) and \\(\\boldsymbol{v}\\) functions of \\(\\boldsymbol{x}\\), and a matrix \\(\\mathbf{A}\\) that is not a function of \\(\\boldsymbol{x}\\) and a non-singular matrix \\(\\mathbf{B}\\) that is a function of the scalar \\(s\\).\n\n\\(\\displaystyle \\frac{\\partial a}{\\partial \\boldsymbol{x}} = \\boldsymbol{0}\\)\n\\(\\displaystyle \\frac{\\partial \\boldsymbol{x}}{\\partial \\boldsymbol{x}} = \\mathbf{I}_m\\)\n\\(\\displaystyle \\frac{\\partial \\boldsymbol{Ax}}{\\partial \\boldsymbol{x}} = \\mathbf{A}^{\\scriptscriptstyle \\top}\\)\n\\(\\displaystyle \\frac{\\partial \\boldsymbol{x}^{\\scriptscriptstyle \\top}\\mathbf{A}}{\\partial \\boldsymbol{x}} = \\mathbf{A}\\)\n\\(\\displaystyle \\frac{\\partial a\\boldsymbol{u}}{\\partial \\boldsymbol{x}} = a\\frac{\\partial \\boldsymbol{u}}{\\partial \\boldsymbol{x}}\\)\n\\(\\displaystyle \\frac{\\partial b\\boldsymbol{u}}{\\partial \\boldsymbol{x}} = b\\frac{\\partial \\boldsymbol{u}}{\\partial \\boldsymbol{x}} + \\frac{\\partial b}{\\partial \\boldsymbol{x}}\\boldsymbol{u}^{\\scriptscriptstyle \\top}\\)\n\\(\\displaystyle \\frac{\\partial \\boldsymbol{Au}}{\\partial \\boldsymbol{x}} = \\frac{\\partial \\boldsymbol{u}}{\\partial \\boldsymbol{x}}\\mathbf{A}^{\\scriptscriptstyle \\top}\\)\n\\(\\displaystyle \\frac{\\partial (\\boldsymbol{u} + \\boldsymbol{v})}{\\partial \\boldsymbol{x}} = \\frac{\\partial \\boldsymbol{u}}{\\partial \\boldsymbol{x}} + \\frac{\\partial \\boldsymbol{v}}{\\partial \\boldsymbol{x}}\\)\n$ = $\n$ = + $\n$ = + ^ $\n\\(\\displaystyle \\frac{\\partial \\mathbf{B}^{-1}}{\\partial s} = -\\mathbf{B}^{-1} \\frac{\\partial \\mathbf{B}}{\\partial s} \\mathbf{B}^{-1}\\)\n\\(\\displaystyle \\frac{\\partial \\log|\\mathbf{B}|}{\\partial s} = {\\rm tr}\\left(\\mathbf{B}^{-1}\\frac{\\partial \\mathbf{B}}{\\partial s}\\right)\\)"
  },
  {
    "objectID": "linear-algebra.html#determinants",
    "href": "linear-algebra.html#determinants",
    "title": "Linear Algebra",
    "section": "Determinants",
    "text": "Determinants\n\nDeterminants of non-square matrix are undefined.\nFor a \\(n\\times n\\) matrix \\(\\mathbf{A}= \\{a_{ij}\\}\\) the determinant can be obtained by expanding by elements of a row: \\[|\\mathbf{A}| = \\sum_{j=1}^n a_{ij} (-1)^{i + j} |\\mathbf{M}_{ij}|\\] for any \\(i = 1, ..., n\\) where the minor of \\(a_{ij}\\), \\(|\\mathbf{M}_{ij}|\\), is the determinant of \\(\\mathbf{A}\\) with rows \\(i\\) and \\(j\\) removed. The determinant can also be obtained by expanding elements of a column in a similar fashion.\nThe signed minor \\((-1)^{i + j} |\\mathbf{M}_{ij}|\\) is referred to as cofactor.\n\\(|\\mathbf{A}\\mathbf{B}| = |\\mathbf{B}\\mathbf{A}|= |\\mathbf{A}||\\mathbf{B}|\\) if \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are square matrices.\nIf \\(\\mathbf{B}\\) is obtained from \\(\\mathbf{A}\\) by swapping two rows then \\(|\\mathbf{A}| = -|\\mathbf{B}|\\).\nIf \\(\\mathbf{B}\\) is obtained from \\(\\mathbf{A}\\) by adding a multiple of a row (column) to a row (column) then \\(|\\mathbf{A}| = |\\mathbf{B}|\\).\nIf \\(\\mathbf{B}\\) is obtained from \\(\\mathbf{A}\\) by taking out a common factor \\(\\lambda\\) from each entry in a row of \\(\\mathbf{A}\\) then \\(|\\mathbf{A}| = \\lambda |\\mathbf{B}|\\).\nIf \\(\\mathbf{A}\\) is a triangular matrix then \\(|\\mathbf{A}| = a_{11}a_{22}...a_{nn}\\).\n\\(|\\mathbf{A}| = |\\mathbf{A}^{\\scriptscriptstyle \\top}|\\).\n\\(|\\mathbf{A}^k| = |\\mathbf{A}|^k\\).\n\\(|\\mathbf{A}^{-1}| = |\\mathbf{A}|^{-1}\\).\nIf \\(\\mathbf{A}\\) is an \\(n\\times n\\) matrix then \\(|k\\mathbf{A}| = k^n|\\mathbf{A}|\\) where \\(k \\in \\mathbb{R}\\).\nIf two rows of \\(\\mathbf{A}\\) are the same, \\(|\\mathbf{A}| = 0\\).\nIf a row of \\(\\mathbf{A}\\) has zero for every element then \\(|\\mathbf{A}| = 0\\).\nFor orthogonal \\(\\mathbf{A}\\), \\(|\\mathbf{A}|=\\pm 1\\) since \\(\\mathbf{A}\\mathbf{A}^{\\scriptscriptstyle \\top}= \\mathbf{I}\\) implies \\(|\\mathbf{A}|^2 = 1\\).\nFor idempotent \\(\\mathbf{A}\\), \\(|\\mathbf{A}|=0\\) or 1 because \\(|\\mathbf{A}|^2 = |\\mathbf{A}|\\).\n\\(|\\mathbf{A}| = \\prod_{i=1}^{n} \\lambda_i\\)\n\\(|\\mathbf{A}\\otimes \\mathbf{B}| = |\\mathbf{A}|^m |\\mathbf{B}|^n\\) where \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are square matrices of size \\(n\\) and \\(m\\), respectively.\nFor square matrices of the same dimension \\(\\mathbf{A}, \\mathbf{B}, \\mathbf{C}\\): \\[\\begin{vmatrix}\n\\mathbf{A}& \\mathbf{0} \\\\\n\\mathbf{B}& \\mathbf{C}\n\\end{vmatrix} = |\\mathbf{A}||\\mathbf{C}|\\quad\\text{and}\\quad\\begin{vmatrix}\\mathbf{0} & \\mathbf{A}\\\\\n-\\mathbf{I} & \\mathbf{B}\n\\end{vmatrix} = |\\mathbf{A}|.\\]\nSylvestor’s theorem for determinants \\[|\\mathbf{A}+ \\mathbf{B}\\mathbf{C}\\mathbf{D}^{\\scriptscriptstyle \\top}| = |\\mathbf{C}^{-1} + \\mathbf{D}^{\\scriptscriptstyle \\top}\\mathbf{A}^{-1}\\mathbf{B}||\\mathbf{A}||\\mathbf{C}|\\]"
  },
  {
    "objectID": "linear-algebra.html#eigen-decomposition",
    "href": "linear-algebra.html#eigen-decomposition",
    "title": "Linear Algebra",
    "section": "Eigen-Decomposition",
    "text": "Eigen-Decomposition\nIf \\(\\mathbf{A}\\) is an \\(n\\times n\\) matrix, \\(\\boldsymbol{x}\\) a non-zero \\(n\\times 1\\) column vector and \\(\\lambda\\) is a scalar such that \\(\\mathbf{A}\\boldsymbol{x}= \\lambda \\boldsymbol{x}\\), we call \\(\\boldsymbol{x}\\) an eigenvector of \\(\\mathbf{A}\\), and \\(\\lambda\\) the corresponding eigenvalue (or \\(\\lambda\\)-eigenvector of \\(A\\)). The set \\(\\{\\boldsymbol{x}\\in \\mathbb{R}^{n} | \\mathbf{A}\\boldsymbol{x}= \\lambda \\boldsymbol{x}\\}\\) is called the \\(\\lambda\\)-eigenspace of \\(\\mathbf{A}\\) and comprises of the \\(\\lambda\\)-eigenvectors and \\(\\boldsymbol{0}\\).\n\nA number \\(\\lambda\\) is an eigenvalue of \\(\\mathbf{A}\\) if and only if \\(|\\mathbf{A}-\\lambda \\mathbf{I}_n| = 0\\).\n\n\nDiagonalizable\nDiagonalizable Theorem: If \\(\\mathbb{R}^n\\) is a basis of \\(\\{\\boldsymbol{v}_1, \\boldsymbol{v}_2, ..., \\boldsymbol{v}_n \\}\\) consisting of eigenvectors of an \\(n\\times n\\) matrix \\(\\mathbf{A}\\) then there exists an invertible matrix \\(\\mathbf{P}\\) and a diagonal matrix \\(\\mathbf{D}\\) such that \\(\\mathbf{D} = \\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}\\). And so \\(\\mathbf{A}^n = \\mathbf{P}\\mathbf{D}^{n}\\mathbf{P}^{-1}\\).\n\nProjection matrix are diagonalizable with 0s and 1s on the diagonal.\nReal symmetric matrices are (orthogonally) diagonalizable by orthogonal matrices so \\(\\mathbf{D} = \\mathbf{Q}^{\\scriptscriptstyle \\top}\\mathbf{A}\\mathbf{Q}\\) where \\(\\mathbf{Q}\\) is an orthogonal matrix.\n\\(\\mathbf{A}\\) is diagonalizable if and only if the sum of the dimensions of its eigenspaces is equal to \\(n\\)."
  },
  {
    "objectID": "linear-algebra.html#illustrations",
    "href": "linear-algebra.html#illustrations",
    "title": "Linear Algebra",
    "section": "Illustrations",
    "text": "Illustrations\n\nGeneration matrix \\(\\mathbf{A}\\): relating the frequencies of mating types in one generation to those in another \\(f^{(i+1)} = \\mathbf{A}f^{(i)}\\).\nMarkov Chain: \\(\\boldsymbol{x}\\) is a state probability vector and \\(\\mathbf{P}\\) is the transition probability matrix which is related by \\(\\boldsymbol{x}_{n+1}^{\\scriptscriptstyle \\top}= \\boldsymbol{x}_n^{\\scriptscriptstyle \\top}\\mathbf{P} = \\boldsymbol{x}_0^{\\scriptscriptstyle \\top}\\mathbf{P}^{n+1}\\). Note \\(\\mathbf{P}^n\\boldsymbol{1} = \\boldsymbol{1}\\).\nLinear Programming: below are equivalent.\n\nminimise \\(f=\\boldsymbol{c}^{\\scriptscriptstyle \\top}\\boldsymbol{x}\\) subject to \\(\\mathbf{A}\\boldsymbol{x}\\geq \\boldsymbol{r}\\) and \\(\\boldsymbol{x}\\geq \\boldsymbol{0}\\)\nmaximise \\(g=\\boldsymbol{r}^{\\scriptscriptstyle \\top}\\boldsymbol{z}\\) subject to \\(\\mathbf{A}\\boldsymbol{z} \\leq \\boldsymbol{r}\\) and \\(\\boldsymbol{z} \\geq \\boldsymbol{0}\\)\n\nGraph Theory: Suppose a set of communication stations \\(\\{S_i\\}\\). \\(\\mathbf{T} = \\{t_{ij}\\}\\) where \\(t_{ij} = 0\\) except \\(t_{ij}=1\\) if a message can be sent from \\(S_i\\) to \\(S_j\\). Then \\(\\mathbf{T}^r = \\{t_{ij}^{(r)}\\}\\), the element \\(t_{ij}^{(r)}\\) is then the number of ways of getting a message from station \\(i\\) to station \\(j\\) in exactly \\(r\\) steps."
  },
  {
    "objectID": "linear-algebra.html#partitioned-matrix",
    "href": "linear-algebra.html#partitioned-matrix",
    "title": "Linear Algebra",
    "section": "Partitioned Matrix",
    "text": "Partitioned Matrix\n\nIf \\(\\mathbf{A}_{r\\times c} = \\begin{bmatrix} \\boldsymbol{\\alpha}^{\\scriptscriptstyle \\top}_1 \\\\ \\vdots \\\\ \\boldsymbol{\\alpha}^{\\scriptscriptstyle \\top}_r \\\\ \\end{bmatrix}\\) and \\(\\mathbf{B}_{c\\times s} = \\begin{bmatrix} \\boldsymbol{b}_1 & \\cdots & \\boldsymbol{b}_s \\end{bmatrix}\\): \\[\\mathbf{A}\\mathbf{B}= \\{\\boldsymbol{\\alpha}^{\\scriptscriptstyle \\top}_i \\boldsymbol{b}_j\\} = \\left\\{\\sum_{k=1}^c a_{ik}b_{kj}\\right\\}\\]\nIf \\(\\mathbf{A}= \\begin{bmatrix} \\mathbf{A}_{11} & \\mathbf{A}_{12} \\\\ \\mathbf{A}_{21} & \\mathbf{A}_{22} \\end{bmatrix}\\) and \\(\\mathbf{B}= \\begin{bmatrix} \\mathbf{B}_{11}\\\\ \\mathbf{B}_{21} \\end{bmatrix}\\) then \\[\\mathbf{A}\\mathbf{B}= \\begin{bmatrix}\\mathbf{A}_{11}\\mathbf{B}_{11} + \\mathbf{A}_{12}\\mathbf{B}_{21}\\\\\n\\mathbf{A}_{21}\\mathbf{B}_{11} + \\mathbf{A}_{22}\\mathbf{B}_{21}\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "linear-algebra.html#non-negative-definite-matrices",
    "href": "linear-algebra.html#non-negative-definite-matrices",
    "title": "Linear Algebra",
    "section": "Non-negative definite matrices",
    "text": "Non-negative definite matrices\n\nPositive definite matrices\nA symmetric \\(n\\times n\\) real matrix \\(\\mathbf{A}\\) is said to be positive definite if $^> 0 $ is positive for every non-zero column vector \\(\\boldsymbol{x}\\).\nA positive definite matrix holds the following properties:\n\nall its eigenvalues are positive\nevery positive definite matrix is invertible and its inverse is also positive definite.\nit has a unique Cholesky decomposition: the matrix \\(\\mathbf{A}\\) is positive definite if and only if there exists a unique lower triangular matrix \\(\\boldsymbol{L}\\), with real and strictly positive diagonal elements, such that \\(\\mathbf{A}= \\boldsymbol{L}\\boldsymbol{L}^{\\scriptscriptstyle \\top}\\).\n\\(\\mathbf{X}^{\\scriptscriptstyle \\top}\\mathbf{A}\\mathbf{X}\\) is positive-semidefinite. If \\(\\mathbf{X}\\) is invertible, then \\(\\mathbf{X}^{\\scriptscriptstyle \\top}\\mathbf{A}\\mathbf{X}\\) is positive definite. Note that \\(\\mathbf{X}^{-1}\\mathbf{A}\\mathbf{X}\\) does not need to be positive definite.\n\n\n\nPositive semi-definite matrices\nA symmetric matrix \\(\\mathbf{A}\\) is positive semi-definite if \\(\\boldsymbol{x}^{\\scriptscriptstyle \\top}\\mathbf{A}\\boldsymbol{x}\\geq 0\\) for all \\(\\boldsymbol{x}\\) and \\(\\boldsymbol{x}^{\\scriptscriptstyle \\top}\\mathbf{A}\\boldsymbol{x}=0\\) for some \\(\\boldsymbol{x}\\neq \\boldsymbol{0}\\).\n\nWhen \\(\\mathbf{A}\\) is p.(s.)d. so is \\(\\mathbf{P}\\mathbf{A}\\mathbf{P}^{\\scriptscriptstyle \\top}\\) for nonsingular \\(\\mathbf{P}\\).\nFor real \\(\\mathbf{X}\\), \\(\\mathbf{X}^{\\scriptscriptstyle \\top}\\mathbf{X}\\) is n.n.d. It is p.d. when \\(\\mathbf{X}\\) has full rank or else it is p.s.d."
  },
  {
    "objectID": "linear-algebra.html#quadratic-form",
    "href": "linear-algebra.html#quadratic-form",
    "title": "Linear Algebra",
    "section": "Quadratic Form",
    "text": "Quadratic Form\n\nThe quadratic form of a matrix is given as \\(\\boldsymbol{x}^{\\scriptscriptstyle \\top}\\mathbf{A}\\boldsymbol{x}= \\sum_i \\sum_j x_i x_j a_{ij}\\).\nFor any particular quadratic form, there is a unique symmetric matrix \\(\\mathbf{A}\\) for which the quadratic form can be expressed as \\(\\boldsymbol{x}^{\\scriptscriptstyle \\top}\\mathbf{A}\\boldsymbol{x}\\).\nIf we have the quadratic form \\(\\boldsymbol{x}^{\\scriptscriptstyle \\top}\\mathbf{A}\\boldsymbol{x}\\) where \\(\\mathbf{A}\\) is not symmetric then we can rewrite it as \\(\\boldsymbol{x}^{\\scriptscriptstyle \\top}\\left[\\frac{1}{2}(\\mathbf{A}+ \\mathbf{A}^{\\scriptscriptstyle \\top})\\right]\\boldsymbol{x}\\) where \\(\\frac{1}{2}(\\mathbf{A}+ \\mathbf{A}^{\\scriptscriptstyle \\top})\\) is a unique symmetric matrix."
  },
  {
    "objectID": "linear-algebra.html#elementary-operator-matrix",
    "href": "linear-algebra.html#elementary-operator-matrix",
    "title": "Linear Algebra",
    "section": "Elementary operator matrix",
    "text": "Elementary operator matrix\nThere are three types of elementary operator matrices: * Adding multiple of a row to another row. Let \\(\\mathbf{E}_{ij}(k)\\) be an identity matrix where the off-diagonal element \\((i,j)\\) is \\(k\\). Then \\(\\mathbf{E}_{ij}(k)\\mathbf{A}\\) is the operation on \\(\\mathbf{A}\\) of adding to its \\(i\\)-th row \\(k\\) times its \\(j\\)-th row. Note \\(|\\mathbf{E}_{ij}(k)| = 1\\). * Interchanging two rows of a matrix. Let \\(\\mathbf{P}_{ij}\\) be an identity matrix with \\(i\\)-th and \\(j\\)-th rows interchanged. Note \\(|\\mathbf{P}_{ij}| = -1\\). * Multiplying a row by a scalar \\(k\\). Let \\(\\mathbf{R}_{ii}(k)\\) be an identity matrix with \\(i\\)-th diagional element replaced by \\(k\\). Note \\(|\\mathbf{R}_{ii}(k)| = k\\)."
  },
  {
    "objectID": "linear-algebra.html#matrices-with-all-element-equal",
    "href": "linear-algebra.html#matrices-with-all-element-equal",
    "title": "Linear Algebra",
    "section": "Matrices with all element equal",
    "text": "Matrices with all element equal\n\n\\(\\boldsymbol{1}_n\\) is a vector of length \\(n\\) with all elements are unity.\n\\(\\mathbf{J}_{r\\times s}\\) is a \\(r\\times s\\) matrix with all elements unity.\nPre (post) multiplication of a matrix \\(\\mathbf{A}\\) by \\(\\boldsymbol{1}_n^{\\scriptscriptstyle \\top}\\) results in a row (column) vector with \\(i\\)-th element equal to the sum of the elements of \\(i\\)-th column (row) of \\(\\mathbf{A}\\).\n\\(\\boldsymbol{1}_n^{\\scriptscriptstyle \\top}\\boldsymbol{1}_n = n\\)\n\\(\\boldsymbol{1}_r\\boldsymbol{1}_s^{\\scriptscriptstyle \\top}= \\mathbf{J}_{r\\times s}\\)\n\\(\\mathbf{J}_{r\\times s}\\mathbf{J}_{s\\times t} = s\\mathbf{J}_{r\\times t}\\)\n\\(\\boldsymbol{1}^{\\scriptscriptstyle \\top}_r \\mathbf{J}_{r\\times s} = r\\boldsymbol{1}_s^{\\scriptscriptstyle \\top}\\)\n\\(\\mathbf{J}_{r\\times s}\\boldsymbol{1}_s = s\\boldsymbol{1}_r\\)\n\\(\\mathbf{J}_n=\\boldsymbol{1}_n\\boldsymbol{1}_n^{\\scriptscriptstyle \\top}\\)\n\\(\\mathbf{J}_n^2 = n\\mathbf{J}_n\\)\n\\(\\bar{\\mathbf{J}}_n = \\frac{1}{n} \\mathbf{J}_n\\)\n\\(\\bar{\\mathbf{J}}_n^2 = \\bar{\\mathbf{J}}_n\\)\n\n\nCentering matrix\n\n\\(\\mathbf{C}_n = \\mathbf{I} - \\bar{\\mathbf{J}}_n\\)\n\\(\\mathbf{C} = \\mathbf{C}^{\\scriptscriptstyle \\top}= \\mathbf{C}^2\\)\n\\(\\mathbf{C}\\boldsymbol{1} = \\boldsymbol{0}\\)\n\\(\\mathbf{C}\\mathbf{J} = \\mathbf{J}\\mathbf{C} = \\boldsymbol{0}\\)\n\\(\\boldsymbol{x}^{\\scriptscriptstyle \\top}\\mathbf{C}\\boldsymbol{x}= \\sum_{i=1}^n (x_i - \\bar{x})^2\\)"
  },
  {
    "objectID": "linear-algebra.html#matrices-with-all-elements-equal",
    "href": "linear-algebra.html#matrices-with-all-elements-equal",
    "title": "Linear Algebra",
    "section": "Matrices with all elements equal",
    "text": "Matrices with all elements equal\n\n\\(\\boldsymbol{1}_n\\) is a vector of length \\(n\\) with all elements are unity.\n\\(\\mathbf{J}_{r\\times s}\\) is a \\(r\\times s\\) matrix with all elements unity.\nPre (post) multiplication of a matrix \\(\\mathbf{A}\\) by \\(\\boldsymbol{1}_n^{\\scriptscriptstyle \\top}\\) results in a row (column) vector with \\(i\\)-th element equal to the sum of the elements of \\(i\\)-th column (row) of \\(\\mathbf{A}\\).\n\\(\\boldsymbol{1}_n^{\\scriptscriptstyle \\top}\\boldsymbol{1}_n = n\\)\n\\(\\boldsymbol{1}_r\\boldsymbol{1}_s^{\\scriptscriptstyle \\top}= \\mathbf{J}_{r\\times s}\\)\n\\(\\mathbf{J}_{r\\times s}\\mathbf{J}_{s\\times t} = s\\mathbf{J}_{r\\times t}\\)\n\\(\\boldsymbol{1}^{\\scriptscriptstyle \\top}_r \\mathbf{J}_{r\\times s} = r\\boldsymbol{1}_s^{\\scriptscriptstyle \\top}\\)\n\\(\\mathbf{J}_{r\\times s}\\boldsymbol{1}_s = s\\boldsymbol{1}_r\\)\n\\(\\mathbf{J}_n=\\boldsymbol{1}_n\\boldsymbol{1}_n^{\\scriptscriptstyle \\top}\\)\n\\(\\mathbf{J}_n^2 = n\\mathbf{J}_n\\)\n\\(\\bar{\\mathbf{J}}_n = \\frac{1}{n} \\mathbf{J}_n\\)\n\\(\\bar{\\mathbf{J}}_n^2 = \\bar{\\mathbf{J}}_n\\)\n\n\nCentering matrix\n\n\\(\\mathbf{C}_n = \\mathbf{I} - \\bar{\\mathbf{J}}_n\\)\n\\(\\mathbf{C} = \\mathbf{C}^{\\scriptscriptstyle \\top}= \\mathbf{C}^2\\)\n\\(\\mathbf{C}\\boldsymbol{1} = \\boldsymbol{0}\\)\n\\(\\mathbf{C}\\mathbf{J} = \\mathbf{J}\\mathbf{C} = \\boldsymbol{0}\\)\n\\(\\boldsymbol{x}^{\\scriptscriptstyle \\top}\\mathbf{C}\\boldsymbol{x}= \\sum_{i=1}^n (x_i - \\bar{x})^2\\)"
  },
  {
    "objectID": "linear-algebra.html#vector-space",
    "href": "linear-algebra.html#vector-space",
    "title": "Linear Algebra",
    "section": "Vector Space",
    "text": "Vector Space\nA vector space over \\(\\mathbb{R}\\) is a non-empty set \\(V\\) whose elements are called vectors on which two operations are defined, namely addition of vectors and multiplication of a vector by a scalar satisfying the 10 axioms below.\n\nAxioms of vector space:\nFor all \\(\\boldsymbol{u}, \\boldsymbol{v}, \\boldsymbol{w} \\in V\\) and \\(k\\), \\(k1\\), \\(k2\\) \\(\\in \\mathbb{R}\\),\n\n[A1] \\(\\boldsymbol{u} + \\boldsymbol{v} \\in V\\). This property is called closure under addition.\n[A2] \\((\\boldsymbol{u} + \\boldsymbol{v}) + \\boldsymbol{w} = \\boldsymbol{u} + (\\boldsymbol{v} + \\boldsymbol{w})\\). This is called .\n[A3] \\(\\boldsymbol{u} + \\boldsymbol{v} = \\boldsymbol{v} + \\boldsymbol{u}\\). That is, addition is commutative.\n[A4] There is a zero vector \\(\\boldsymbol{0} \\in V\\) with \\(\\boldsymbol{v} + \\boldsymbol{0} = \\boldsymbol{0} + \\boldsymbol{v} = \\boldsymbol{v}\\).\n[A5] There is a vector which we write as \\(-\\boldsymbol{v}\\) and call a negative of , such that \\(\\boldsymbol{v} + -\\boldsymbol{v} = \\boldsymbol{0}\\).\n[S1] \\(k\\boldsymbol{v} \\in V\\). This property is called closure under multiplication by a scalar.\n[S2] \\(k(\\boldsymbol{u} + \\boldsymbol{v}) = k\\boldsymbol{u} + k\\boldsymbol{v}\\).\n[S3] \\((k_1 + k_2) \\boldsymbol{v} = k_1\\boldsymbol{v} + k_2 \\boldsymbol{v}\\).\n[S4] \\((k_1 k_2)\\boldsymbol{v} = k_1(k_2\\boldsymbol{v})\\).\n[S5] \\(1\\boldsymbol{v} = \\boldsymbol{v}\\).\n\n\n\nSpecial set of vectors\nThe set of vectors \\(X=\\{\\boldsymbol{v}_1, \\boldsymbol{v}_2, ..., \\boldsymbol{v}_n \\}\\) in a vector space \\(V\\) is called a linearly independent set if the only scalars that satisfy \\(\\sum_{i=1}^n a_i\\boldsymbol{v}_i = \\boldsymbol{0}\\) are \\(a_1 = a_2 = ... = a_n = 0\\), otherwise it is called a linearly dependent set. \\(\\mathcal{R}(X)\\) is a subspace of \\(V\\) and is the smallest subspace of \\(V\\) containing the set \\(X\\).\n\n\nBasis\nA set of vectors \\(X\\) in a vector space \\(V\\) is called a basis of \\(V\\) if \\(\\{\\boldsymbol{v}_1, \\boldsymbol{v}_2, ..., \\boldsymbol{v}_n \\}\\) is a linearly independent set and \\(\\mathcal{R}(X) = V\\).\n\nIf an \\(n\\times n\\) matrix \\(\\boldsymbol{M}\\) has \\(n\\) distinct eigenvalues then the set of \\(n\\) eigenvectors formed by selecting a non-zero vector from each eigenspace is a basis of \\(\\mathbb{R}^n\\).\nIf \\(\\{\\boldsymbol{v}_1, \\boldsymbol{v}_2, ..., \\boldsymbol{v}_n \\}\\) is a basis of a vector space \\(V\\) then each vector \\(\\boldsymbol{v}\\) in \\(V\\) can be expressed as a linear combination of the basis in .\nIf one basis of a vector space \\(V\\) contains \\(n\\) vectors then every basis of \\(V\\) contains \\(n\\) vectors.\nIf \\(W\\) is a non-zero subspace of \\(V\\) and \\(V\\) has dimension \\(n\\) then \\({\\rm dim}\\left(W\\right)\\) \\(\\leq n\\) with \\({\\rm dim}\\left(W\\right)\\) \\(=n\\) if and only if \\(W=V\\).\nIn a vector space \\(V\\) of dimension \\(n\\), every linearly independent set containing fewer than \\(n\\) vectors can be extended to a basis of \\(V\\), and every spanning set of \\(V\\) with more than \\(n\\) vectors contains a basis of \\(V\\) where a spanning set is the span of the set of vectors in \\(V\\). Moreover, every linearly independent set of \\(n\\) vectors is a basis of \\(V\\) and every set of \\(n\\) vectors which spans \\(V\\) is a basis of \\(V\\). \\end{itemize}\n\n\n\nVector subspaces\nLet \\(S\\) be a non-empty subset of a vector space \\(V\\). If \\(S\\) itself satisfies the 10 vector space axioms with the same operations of addition and multiplication by a scalar then \\(S\\) is called a subspace of \\(V\\).\nA subset \\(S\\) of a vector space \\(V\\) is a subspace of \\(V\\) if \\(S\\) is not empty and \\(S\\) satisfies A1 and S1. If the zero vector of \\(V\\) is not in \\(S\\) then \\(S\\) is not a subspace.\nThe addition of vector spaces is defined below: \\[V + U = \\{\\boldsymbol{v} + \\boldsymbol{u}: \\boldsymbol{v}\\in V,\\boldsymbol{w}\\in U\\}.\\] A decomposition of a vector space \\(W\\) to \\(V\\) and \\(U\\) is written as \\[  W = V \\oplus U\\] where \\(V \\cap U = \\{\\boldsymbol{0}\\}\\). We also say \\(W\\) is the direct sum of \\(V\\) and \\(U\\)."
  },
  {
    "objectID": "linear-algebra.html#diagonal-matrix",
    "href": "linear-algebra.html#diagonal-matrix",
    "title": "Linear Algebra",
    "section": "Diagonal Matrix",
    "text": "Diagonal Matrix\n\n\\(\\mathbf{D}\\mathbf{A}\\) (or \\(\\mathbf{A}\\mathbf{D}\\)) gives a matrix whose rows (or columns) are those of \\(\\mathbf{A}\\) multiplied by the respective diagonal elements of \\(\\mathbf{D}\\)"
  },
  {
    "objectID": "linear-algebra.html#triangular-matrix",
    "href": "linear-algebra.html#triangular-matrix",
    "title": "Linear Algebra",
    "section": "Triangular Matrix",
    "text": "Triangular Matrix\n\nA square matrix with all elements above (or below) the diagonal are zero is called a lower (upper) triangular matrix."
  },
  {
    "objectID": "linear-algebra.html#transition-probability-matrix",
    "href": "linear-algebra.html#transition-probability-matrix",
    "title": "Linear Algebra",
    "section": "Transition Probability Matrix",
    "text": "Transition Probability Matrix\n\nThe \\((i,j)\\)-th element of a transition probability matirx, \\(p_ij\\), is the probability of the probability of transitioning from state \\(i\\) to state \\(j\\) in a pre-defined time period.\nThe sum of the elements of each row is 1.\nFor a transition probability matrix \\(\\mathbf{P}\\), \\(\\mathbf{P}^k\\) is also a transition proability matrix with the elements correspond to the transition from state \\(i\\) to state \\(j\\) in \\(k\\) time periods.\n\nExamples\n\nFeller (1968, p. 439) for a transition proability matrix \\[\\mathbf{P} = \\begin{bmatrix}\n\\mathbf{A}& \\mathbf{0} & \\mathbf{0} \\\\\n\\mathbf{0} & \\mathbf{B}& \\mathbf{0} \\\\\n\\mathbf{U}_1 & \\mathbf{V}_1 & \\mathbf{T}\n\\end{bmatrix}\\] where \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are also transition probality matrices. Then for \\(\\mathbf{U}_n = \\mathbf{U}_1\\mathbf{A}^{n - 1} + \\mathbf{T}\\mathbf{U}_{n-1}\\) and \\(\\mathbf{V}_n = \\mathbf{V}_1\\mathbf{B}^{n - 1} + \\mathbf{T}\\mathbf{V}_{n-1}\\), we have \\[\\mathbf{P}^n = \\begin{bmatrix}\\mathbf{A}^n & \\mathbf{0} & \\mathbf{0}\\\\\n\\mathbf{0} & \\mathbf{B}^n & \\mathbf{0} \\\\\n\\mathbf{U}_n & \\mathbf{V}_n & \\mathbf{T}^n \\end{bmatrix}\\]"
  },
  {
    "objectID": "linear-algebra.html#orthogonal-matrix",
    "href": "linear-algebra.html#orthogonal-matrix",
    "title": "Linear Algebra",
    "section": "Orthogonal Matrix",
    "text": "Orthogonal Matrix\n\nIf any two of the following conditions are satisfied then the matrix \\(\\mathbf{A}\\) is an orthogonal matrix:\n\n\\(\\mathbf{A}\\) is square\n\\(\\mathbf{A}^{\\scriptscriptstyle \\top}\\mathbf{A}= \\mathbf{I}\\)\n\\(\\mathbf{A}\\mathbf{A}^{\\scriptscriptstyle \\top}= \\mathbf{I}\\)\n\\(|\\mathbf{A}| = \\pm 1\\)\n\n\\(\\mathbf{A}^{-1} = \\mathbf{A}^{\\scriptscriptstyle \\top}\\)\n\n\nSpecial Cases\n\nHelmert matrix \\(\\mathbf{H}_n\\) of order \\(n\\) has \\(n^{-\\frac{1}{2}}\\boldsymbol{1}_n^{\\scriptscriptstyle \\top}\\) for its first row and the other \\(i\\)-th row is given by \\[\\left(i(i-1)\\right)^{-\\frac{1}{2}}\\begin{bmatrix}\\boldsymbol{1}_{i - 1}^{\\scriptscriptstyle \\top}& -(i-1) & \\boldsymbol{0}_{1\\times (n-i)}\\end{bmatrix}.\\]\n\n\nExample\n\n\\[\\mathbf{H}_4 = \\begin{bmatrix}\n1/\\sqrt{4} & 1/\\sqrt{4} & 1/\\sqrt{4} & 1/\\sqrt{4}\\\\\n1/\\sqrt{2} & -1/\\sqrt{2} & 0          & 0\\\\\n1/\\sqrt{6} & 1/\\sqrt{6} & -2/\\sqrt{6} & 0 \\\\\n1/\\sqrt{12} & 1/\\sqrt{12} & 1/\\sqrt{12} & -3/\\sqrt{12}\n\\end{bmatrix}\\]\n\nGivens matrices \\(\\mathbf{G}_{rs}\\) or order \\(n\\) is an identity matrix except for four elements: \\(g_{rr}=g_{ss}=\\cos\\theta_{rs}\\) and for \\(r > s\\), \\(-g_{rs}=g_{sr} = \\sin\\theta_{rs}\\).\n\n\nExamples\n\nGivens matrix of order 2 \\[\\begin{bmatrix}\n\\cos\\theta & \\sin\\theta \\\\\n-\\sin\\theta & \\cos\\theta\n\\end{bmatrix}\\] Givens matrix of order 3 \\[\n\\mathbf{G}_{12} = \\begin{bmatrix}\n\\cos \\alpha & \\sin \\alpha & 0 \\\\\n-\\sin \\alpha & \\cos\\alpha & 0 \\\\\n0 & 0 & 1\\end{bmatrix}\\quad\n\\mathbf{G}_{13} = \\begin{bmatrix}\n\\cos\\beta & 0 & \\sin\\beta\\\\\n0 & 1 & 0 \\\\\n-\\sin\\beta & 0 & \\cos\\beta\n\\end{bmatrix}\\quad\n\\mathbf{G}_{23}= \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & \\cos \\gamma & \\sin\\gamma\\\\\n0 & -\\sin\\gamma & \\cos\\gamma\n\\end{bmatrix}\n\\]\n\nPremultiplying by a Givens matrix will make the result an upper triangle. Postmultiplying by a Givens matrix will make the result a lower triangle. These tranformations are sometimes referred to as a Givens transformation.\n*Household matrices** are orthogonal matrices such that \\[\\mathbf{H} = \\mathbf{I} - 2\\boldsymbol{h}\\boldsymbol{h}^{\\scriptscriptstyle \\top}\\quad\\text{for }\\boldsymbol{h}^{\\scriptscriptstyle \\top}\\boldsymbol{h}=1.\\] For any non-null vector \\(\\boldsymbol{x}\\) there exists \\(\\boldsymbol{h}\\) such that \\(\\mathbf{H}\\boldsymbol{x} = (\\lambda, 0, ..., 0)^{\\scriptscriptstyle \\top}\\) for \\(\\lambda = -(\\text{sign of }x_1)\\sqrt{\\boldsymbol{x}^{\\scriptscriptstyle \\top}\\boldsymbol{x}}\\), \\(h_1=\\sqrt{\\frac{1}{2}(1 - x_1/\\lambda)}\\) and \\(h_i=-x_i/(2h_1\\lambda)\\) for \\(i=2, ..., n\\)."
  },
  {
    "objectID": "linear-algebra.html#null-or-zero-matrices",
    "href": "linear-algebra.html#null-or-zero-matrices",
    "title": "Linear Algebra",
    "section": "Null or Zero Matrices",
    "text": "Null or Zero Matrices\nA matrix is a null or zero matrix if every element is zero."
  },
  {
    "objectID": "linear-algebra.html#identity-matrices",
    "href": "linear-algebra.html#identity-matrices",
    "title": "Linear Algebra",
    "section": "Identity Matrices",
    "text": "Identity Matrices\n\nA diagonal matrix with all diagonal elements equal to unity is called an identity matrix.\nAn identity matrix is denoted with letter \\(\\mathbf{I}\\) with a subscript for its order.\n\\(\\mathbf{I}_r\\mathbf{A}_{r\\times c} = \\mathbf{A}_{r\\times c} \\mathbf{I}_c = \\mathbf{A}_{r\\times c}\\)."
  },
  {
    "objectID": "linear-algebra.html#symmetric-matrices",
    "href": "linear-algebra.html#symmetric-matrices",
    "title": "Linear Algebra",
    "section": "Symmetric Matrices",
    "text": "Symmetric Matrices\n\n\\(\\mathbf{A}\\) is symmetric if \\(\\mathbf{A}= \\mathbf{A}^{\\scriptscriptstyle \\top}\\).\n\\(\\mathbf{A}\\mathbf{A}^{\\scriptscriptstyle \\top}\\) and \\(\\mathbf{A}^{\\scriptscriptstyle \\top}\\mathbf{A}\\) are symmetric.\n\\(\\mathbf{A}^{\\scriptscriptstyle \\top}\\mathbf{A}= \\boldsymbol{0}\\) implies \\(\\mathbf{A}= \\boldsymbol{0}\\).\n\\(\\text{tr}\\left(\\mathbf{A}^{\\scriptscriptstyle \\top}\\mathbf{A}\\right) = \\boldsymbol{0}\\) implies \\(\\mathbf{A}= \\boldsymbol{0}\\).\nIf \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are symmetric matrices, then in general \\(\\mathbf{A}\\mathbf{B}\\) is not a symmetric matrix.\n\n\nProof\n\n\\[(\\mathbf{A}\\mathbf{B})^{\\scriptscriptstyle \\top}= \\mathbf{B}^{\\scriptscriptstyle \\top}\\mathbf{A}^{\\scriptscriptstyle \\top}= \\mathbf{B}\\mathbf{A}.\\] Since \\(\\mathbf{B}\\mathbf{A}\\) is generally not the same as \\(\\mathbf{A}\\mathbf{B}\\), this means \\(\\mathbf{A}\\mathbf{B}\\) is generally not symmetric."
  },
  {
    "objectID": "linear-algebra.html#skew-symmetric-matrices",
    "href": "linear-algebra.html#skew-symmetric-matrices",
    "title": "Linear Algebra",
    "section": "Skew-symmetric matrices",
    "text": "Skew-symmetric matrices\n\nIf a matrix \\(\\mathbf{A}\\) is skew-symmetric, then \\(\\mathbf{A}^{\\scriptscriptstyle \\top}= - \\mathbf{A}\\).\nThe diagonal elements of a skew-symmetric matrices is zero."
  },
  {
    "objectID": "linear-algebra.html#matrix-factorisation",
    "href": "linear-algebra.html#matrix-factorisation",
    "title": "Linear Algebra",
    "section": "Matrix Factorisation",
    "text": "Matrix Factorisation\n\nSuppose a non-full rank matrix \\(\\mathbf{A}= \\begin{bmatrix}  \\mathbf{X}& \\mathbf{Y}\\\\  \\mathbf{Z} & \\boldsymbol{W}  \\end{bmatrix}\\) where \\(\\mathbf{X}\\) is full rank. Then \\[\\mathbf{A}= \\begin{bmatrix}\n  \\mathbf{I} \\\\\n  \\mathbf{Z}\\mathbf{X}^{-1}\n  \\end{bmatrix} \\mathbf{X}\\begin{bmatrix}\n  \\mathbf{I} & \\mathbf{X}^{-1}\\mathbf{Y}\n  \\end{bmatrix}\\]\nIf matrix \\(\\mathbf{A}\\) is not in above form then there exists permutation matrices \\(\\mathbf{P}\\) and \\(\\mathbf{Q}\\) such that \\(\\mathbf{P}\\mathbf{A}\\mathbf{Q}\\) is in the above form."
  },
  {
    "objectID": "linear-algebra.html#solving-linear-equations",
    "href": "linear-algebra.html#solving-linear-equations",
    "title": "Linear Algebra",
    "section": "Solving Linear Equations",
    "text": "Solving Linear Equations\n\nThe consistent equations \\(\\mathbf{A}\\boldsymbol{x}= \\boldsymbol{y}\\) for \\(\\boldsymbol{y}\\neq \\boldsymbol{0}\\) have a solution \\(\\boldsymbol{x}= \\mathbf{G}\\boldsymbol{y}\\) if and only if \\(\\mathbf{A}\\mathbf{G}\\mathbf{A}=\\mathbf{A}\\).\n\\(\\mathbf{A}\\boldsymbol{x}= \\boldsymbol{y}\\) have solutions \\[\\tilde{\\boldsymbol{x}} = \\mathbf{G}\\boldsymbol{y}+ (\\mathbf{G}\\mathbf{A}- \\mathbf{I})\\boldsymbol{z} \\] for \\(\\mathbf{G}=\\mathbf{A}^-\\) and any arbitrary vector \\(\\boldsymbol{z}\\)."
  },
  {
    "objectID": "linear-algebra.html#direct-sum",
    "href": "linear-algebra.html#direct-sum",
    "title": "Linear Algebra",
    "section": "Direct Sum",
    "text": "Direct Sum\n\n\\(\\mathbf{A}\\oplus \\mathbf{B}= \\begin{bmatrix}  \\mathbf{A}& \\boldsymbol{0} \\\\  \\boldsymbol{0} & \\mathbf{B}\\end{bmatrix}\\)\n\\((\\mathbf{A}\\oplus \\mathbf{B}) + (\\mathbf{C} \\oplus \\mathbf{D}) = (\\mathbf{A}+ \\mathbf{C}) \\oplus (\\mathbf{B}+ \\mathbf{D})\\)\n\\((\\mathbf{A}\\oplus \\mathbf{B})(\\mathbf{C} \\oplus \\mathbf{D}) = \\mathbf{A}\\mathbf{C} \\oplus \\mathbf{B}\\mathbf{D}\\)\n\\((\\mathbf{A}\\oplus \\mathbf{B})^{-1} = \\mathbf{A}^{-1}\\oplus \\mathbf{B}^{-1}\\)"
  },
  {
    "objectID": "linear-algebra.html#direct-product",
    "href": "linear-algebra.html#direct-product",
    "title": "Linear Algebra",
    "section": "Direct Product",
    "text": "Direct Product\n\n\\((\\mathbf{A}\\otimes \\mathbf{B})^{\\scriptscriptstyle \\top}= \\mathbf{A}^{\\scriptscriptstyle \\top}\\otimes \\mathbf{B}^{\\scriptscriptstyle \\top}\\)\nFor vectors \\(\\boldsymbol{x}\\) and \\(\\boldsymbol{y}\\): \\(\\boldsymbol{x}^{\\scriptscriptstyle \\top}\\otimes \\boldsymbol{y}= \\boldsymbol{y}\\boldsymbol{x}^{\\scriptscriptstyle \\top}= \\boldsymbol{y}\\otimes \\boldsymbol{x}^{\\scriptscriptstyle \\top}\\)\n\\(\\begin{bmatrix}  \\mathbf{A}_1 & \\mathbf{A}_2  \\end{bmatrix} \\otimes \\mathbf{B}= \\begin{bmatrix}  \\mathbf{A}_1 \\otimes \\mathbf{B}& \\mathbf{A}_2 \\otimes \\mathbf{B}\\end{bmatrix}\\)\n\\((\\mathbf{A}\\otimes \\mathbf{B})(\\mathbf{X}\\otimes \\mathbf{Y}) = \\mathbf{A}\\mathbf{X}\\otimes \\mathbf{B}\\mathbf{Y}\\)\n\\((\\mathbf{A}\\otimes \\mathbf{B})^{-1} = \\mathbf{A}^{-1}\\otimes \\mathbf{B}^{-1}\\)\n\\({\\rm tr}\\left(\\mathbf{A}\\otimes \\mathbf{B}\\right) = {\\rm tr}\\left(\\mathbf{A}\\right){\\rm tr}\\left(\\mathbf{B}\\right)\\)\n\\(|\\mathbf{A}_{p\\times p}\\otimes \\mathbf{B}_{m\\times m }| = |\\mathbf{A}|^m|\\mathbf{B}|^p\\)\nEigenvalues of \\(\\mathbf{A}\\otimes \\mathbf{B}\\) are products of eigenvalues of \\(\\mathbf{A}\\) with those of \\(\\mathbf{B}\\)."
  },
  {
    "objectID": "linear-algebra.html#the-matrix-mathbfxscriptscriptstyle-topmathbfx",
    "href": "linear-algebra.html#the-matrix-mathbfxscriptscriptstyle-topmathbfx",
    "title": "Linear Algebra",
    "section": "The matrix \\(\\mathbf{X}^{\\scriptscriptstyle \\top}\\mathbf{X}\\)",
    "text": "The matrix \\(\\mathbf{X}^{\\scriptscriptstyle \\top}\\mathbf{X}\\)\n\n\\(\\mathbf{X}^{\\scriptscriptstyle \\top}\\mathbf{X}\\) and \\(\\mathbf{X}\\mathbf{X}^{\\scriptscriptstyle \\top}\\) are symmetric matrices.\nIf \\(\\mathbf{X}^{\\scriptscriptstyle \\top}\\mathbf{X}= \\mathbf{0}\\) then \\(\\mathbf{A}= \\mathbf{0}\\).\n\\(\\left((\\mathbf{X}^{\\scriptscriptstyle \\top}\\mathbf{X})^-\\right)^{\\scriptscriptstyle \\top}\\) is a generalised inverse of \\(\\mathbf{X}^{\\scriptscriptstyle \\top}\\mathbf{X}\\).\nIf \\(\\mathbf{P}\\mathbf{X}\\mathbf{X}^{\\scriptscriptstyle \\top}= \\mathbf{Q}\\mathbf{X}\\mathbf{X}^{\\scriptscriptstyle \\top}\\) then \\(\\mathbf{P}\\mathbf{X}= \\mathbf{Q}\\mathbf{X}\\).\n\n\nProof\n\nObserve that \\[(\\mathbf{P}\\mathbf{X}\\mathbf{X}^{\\scriptscriptstyle \\top}- \\mathbf{Q}\\mathbf{X}\\mathbf{X}^{\\scriptscriptstyle \\top})(\\mathbf{P}^{\\scriptscriptstyle \\top}- \\mathbf{Q}^{\\scriptscriptstyle \\top}) \\equiv (\\mathbf{P}\\mathbf{X}- \\mathbf{Q}\\mathbf{X})(\\mathbf{P}\\mathbf{X}- \\mathbf{Q}\\mathbf{X})^{\\scriptscriptstyle \\top}.\\] Hence if \\(\\mathbf{P}\\mathbf{X}\\mathbf{X}^{\\scriptscriptstyle \\top}= \\mathbf{Q}\\mathbf{X}\\mathbf{X}^{\\scriptscriptstyle \\top}\\), then \\(\\mathbf{P}\\mathbf{X}= \\mathbf{Q}\\mathbf{X}\\).\n\n\\(\\mathbf{X}(\\mathbf{X}^{\\scriptscriptstyle \\top}\\mathbf{X})^-\\mathbf{X}^{\\scriptscriptstyle \\top}\\mathbf{X}= \\mathbf{X}\\), i.e. \\((\\mathbf{X}^{\\scriptscriptstyle \\top}\\mathbf{X})^-\\mathbf{X}^{\\scriptscriptstyle \\top}\\) is a generalised inverse of \\(\\mathbf{X}\\).\n\\(\\mathbf{X}(\\mathbf{X}^{\\scriptscriptstyle \\top}\\mathbf{X})^-\\mathbf{X}^{\\scriptscriptstyle \\top}\\) is invariant to the choice of \\((\\mathbf{X}^{\\scriptscriptstyle \\top}\\mathbf{X})^-\\).\n\\(\\mathbf{X}(\\mathbf{X}^{\\scriptscriptstyle \\top}\\mathbf{X})^-\\mathbf{X}^{\\scriptscriptstyle \\top}\\) is symmetric whether \\((\\mathbf{X}^{\\scriptscriptstyle \\top}\\mathbf{X})^-\\) is or not."
  },
  {
    "objectID": "linear-algebra.html#least-squares-equations",
    "href": "linear-algebra.html#least-squares-equations",
    "title": "Linear Algebra",
    "section": "Least Squares Equations",
    "text": "Least Squares Equations\nThe following are invariant to the choice of \\(\\left(\\mathbf{X}^{\\scriptscriptstyle \\top}\\mathbf{X}\\right)^-\\):\n\nthe vector of predicted values \\(\\hat{\\boldsymbol{y}} = \\mathbf{X}\\left(\\mathbf{X}^{\\scriptscriptstyle \\top}\\mathbf{X}\\right)^-\\mathbf{X}^{\\scriptscriptstyle \\top}\\boldsymbol{y}\\);\nThe residual sum of squares \\((\\boldsymbol{y}- \\hat{\\boldsymbol{y}})^{\\scriptscriptstyle \\top}(\\boldsymbol{y}- \\hat{\\boldsymbol{y}})\\)."
  },
  {
    "objectID": "linear-algebra.html#elements-of-vectors",
    "href": "linear-algebra.html#elements-of-vectors",
    "title": "Linear Algebra",
    "section": "Elements of vectors",
    "text": "Elements of vectors\n\nInner product \\(\\boldsymbol{x}^{\\scriptscriptstyle \\top}\\boldsymbol{y}\\)\nOuter product \\(\\boldsymbol{x}\\boldsymbol{y}^{\\scriptscriptstyle \\top}\\)\nElementary vector of length \\(n\\), \\(\\boldsymbol{e}_i\\), is the \\(i\\)-th column of the identity matrix \\(\\mathbf{I}_n\\).\n\\(\\mathbf{I}_n = \\sum_{i=1}^n \\boldsymbol{e}_i\\boldsymbol{e}_i^{\\scriptscriptstyle \\top}\\).\n\\(\\mathbf{E}_{12} = \\boldsymbol{e}_1\\boldsymbol{e}_2^{\\scriptscriptstyle \\top}= \\begin{bmatrix}0 & 1 & 0 \\\\0 & 0 & 0 \\\\ 0 & 0 & 0\\end{bmatrix}\\)\nA norm of a vector \\(\\boldsymbol{x}\\) is given as \\(\\sqrt{\\boldsymbol{x}^{\\scriptscriptstyle \\top}\\boldsymbol{x}}\\).\nA vector is said to be either normal or a unit vector if its norm is unity.\nNon-null vectors \\(\\boldsymbol{x}\\) and \\(\\boldsymbol{y}\\) are orthogonal if \\(\\boldsymbol{x}^{\\scriptscriptstyle \\top}\\boldsymbol{y}= 0\\).\nTwo vectors are orthonormal vectors if they are orthogonal and normal."
  },
  {
    "objectID": "linear-algebra.html#elements-of-matrices",
    "href": "linear-algebra.html#elements-of-matrices",
    "title": "Linear Algebra",
    "section": "Elements of matrices",
    "text": "Elements of matrices\n\nDimension/Order of a matrix\n\nThe dimension or order of a matrix refer to the size of the matrix (i.e. the number of rows and the number of columns).\nA matrix with \\(r\\) rows and \\(c\\) columns has order \\(r\\times c\\). When \\(r=c\\), it can simply be said that the matrix has order \\(r\\).\n\n\n\nDiagonal and off-diagonal elements\n\nFor a square matrix \\(\\mathbf{A}_{r\\times r}\\), the elements \\(a_{11}, a_{22}, ..., a_{rr}\\) are referred to as diagonal elements.\nThe elements of a square matrix that lie in a line parallel to and just below the diagonal, i.e. \\(a_{1,2}, ...,a_{i,j+1}, ..., a_{r-1,r}\\) and \\(a_{2,1}, ...,a_{i,j-1}, ..., a_{r,r-1}\\), are referred to as subdiagonal elements.\nThe elements of square matrix that are not diagonal elements are referred to as off-diagonal or non-diagonal elements.\n\n\n\nThe laws of algebra\nAssume all matrices are conformable for the operations performed.\n\nAssociative laws of addition: \\((\\mathbf{A}+ \\mathbf{B}) + \\mathbf{C}= \\mathbf{A}+ (\\mathbf{B}+ \\mathbf{C})\\).\nAssociative laws of products: \\((\\mathbf{A}\\mathbf{B})\\mathbf{C}= \\mathbf{A}(\\mathbf{B}\\mathbf{C})\\).\nDistributive law: \\(\\mathbf{A}(\\mathbf{B}+ \\mathbf{C}) = \\mathbf{A}\\mathbf{B}+ \\mathbf{A}\\mathbf{C}\\).\nCommutative law of addition: \\(\\mathbf{A}+ \\mathbf{B}= \\mathbf{B}+ \\mathbf{A}\\).\nCommutative law of products do not hold."
  },
  {
    "objectID": "linear-algebra.html#rangespancolumn-space",
    "href": "linear-algebra.html#rangespancolumn-space",
    "title": "Linear Algebra",
    "section": "Range/Span/Column Space",
    "text": "Range/Span/Column Space\nRange, span or column space of \\(\\mathbf{X}\\) is denoted \\(\\mathcal{R}(\\mathbf{X})\\) is the space or set of all possible linear combinations of the columns of \\(\\mathbf{X}\\). Thus \\(\\mathbf{A}\\in \\mathcal{R}(\\mathbf{X})\\) if \\(\\mathbf{A}=\\mathbf{X}\\boldsymbol{b}\\) for some \\(\\boldsymbol{b}\\)."
  },
  {
    "objectID": "linear-algebra.html#kernelnull-space",
    "href": "linear-algebra.html#kernelnull-space",
    "title": "Linear Algebra",
    "section": "Kernel/Null space",
    "text": "Kernel/Null space\nKernel or null space os a matrix \\(\\mathbf{X}\\) is denoted \\(\\mathcal{N}(\\mathbf{X})\\) is the space of all possible linear combinations of vectors orthogonal to the columns of \\(\\mathbf{X}\\)."
  },
  {
    "objectID": "linear-algebra.html#orthogonal-complement",
    "href": "linear-algebra.html#orthogonal-complement",
    "title": "Linear Algebra",
    "section": "Orthogonal complement",
    "text": "Orthogonal complement\nLet \\(V\\) be a finite dimensional vector space and \\(W\\) is a subspace of \\(V\\). Then the orthogonal complement of \\(W\\), denoted \\(W^\\perp\\), is the set of vectors \\[\\{\\boldsymbol{v}\\in V: \\boldsymbol{v}^{\\scriptscriptstyle \\top}\\boldsymbol{w}=0 \\text{ for all } \\boldsymbol{w}\\in W\\}.\\]\n\n\\(W^\\perp\\) is also a subspace of \\(V\\)\n\\((W^\\perp)^\\perp = W\\)\n\\(\\text{dim}(W^\\perp) = \\text{dim} V - \\text{dim} W\\)\n\\(V=W \\oplus W^\\perp\\)"
  },
  {
    "objectID": "linear-algebra.html#rank",
    "href": "linear-algebra.html#rank",
    "title": "Linear Algebra",
    "section": "Rank",
    "text": "Rank\nThe rank of the matrix \\(\\mathbf{X}\\) or the dimension of \\(\\mathcal{R}(\\mathbf{X})\\), written as \\(\\text{rank}\\left(\\mathbf{X}\\right)\\) or \\(\\text{dim}\\left(\\mathcal{R}(\\mathbf{X})\\right)\\), is the number in the minimal (linearly independent) set of columns of \\(\\mathbf{X}\\) that span \\(\\mathcal{R}(\\mathbf{X})\\). Similar definition applies to any vector space \\(V\\), where the dimension of \\(V\\) is the number of the vectors in any basis of \\(V\\).\nSuppose that \\(\\mathbf{A}\\) is an \\(m\\times n\\) matrix then\n\n\\({\\rm rank}\\left(\\mathbf{A}\\right) \\leq \\min (m,n)\\)\nIf \\({\\rm rank}\\left(\\mathbf{A}\\right) = \\min (m,n)\\) then the matrix \\(\\mathbf{A}\\) is said to have .\nOnly a zero matrix has rank zero.\nIf \\(\\mathbf{A}\\) is a square matrix (\\(m=n\\)) then \\(\\mathbf{A}\\) is invertible if and only if \\(\\mathbf{A}\\) has rank \\(n\\).\nIf \\(\\mathbf{B}\\) is any \\(n\\times k\\) matrix, then \\({\\rm rank}\\left(\\mathbf{A}\\mathbf{B}\\right) \\leq \\min({\\rm rank}\\left(\\mathbf{A}\\right), {\\rm rank}\\left(\\mathbf{B}\\right))\\).\nIf \\(\\mathbf{B}\\) is any \\(n\\times k\\) matrix of rank \\(n\\), then \\({\\rm rank}\\left(\\mathbf{A}\\mathbf{B}\\right) = {\\rm rank}\\left(\\mathbf{A}\\right)\\).\nIf \\(\\mathbf{C}\\) is any \\(l\\times m\\) matrix of rank \\(m\\), then \\({\\rm rank}\\left(\\mathbf{C}\\mathbf{A}\\right) = {\\rm rank}\\left(\\mathbf{A}\\right)\\).\nThe \\({\\rm rank}\\left(\\mathbf{A}\\right)=r\\) if and only if there exists an invertible \\(m\\times m\\) matrix \\(\\mathbf{X}\\) and an invertible \\(n\\times n\\) matrix \\(\\mathbf{Y}\\) such that \\(\\mathbf{X}\\mathbf{A}\\mathbf{Y}= \\begin{bmatrix}  \\mathbf{I}_r & \\mathbf{0} \\\\  \\mathbf{0} & \\mathbf{0}  \\end{bmatrix}\\).\n\\({\\rm rank}\\left(\\mathbf{A}+ \\mathbf{B}\\right) \\leq {\\rm rank}\\left(\\begin{bmatrix}  \\mathbf{A}& \\mathbf{B}\\end{bmatrix}\\right) \\leq {\\rm rank}\\left(\\mathbf{A}\\right) + {\\rm rank}\\left(\\mathbf{B}\\right)\\)\nSylvestor’s rank of nullity: If \\(\\mathbf{A}\\) is an \\(m\\times n\\) matrix and \\(\\mathbf{B}\\) is any \\(n\\times k\\) matrix, \\({\\rm rank}\\left(A\\right) + {\\rm rank}\\left(B\\right) - n \\leq {\\rm rank}\\left(AB\\right)\\).\nThe inequality due to Frobenius: \\({\\rm rank}\\left(AB\\right) + {\\rm rank}\\left(BC\\right) \\leq {\\rm rank}\\left(B\\right) + {\\rm rank}\\left(ABC\\right)\\).\nIf \\(\\mathbf{A}\\mathbf{B}\\mathbf{A}= \\mathbf{A}\\) then \\({\\rm rank}\\left(\\mathbf{B}\\mathbf{A}\\right) = {\\rm rank}\\left(\\mathbf{A}\\right)\\)\nRank-nullity Theorem: The rank of a matrix plus the nullity of the matrix equals the number of columns of the matrix, i.e. \\[{\\rm dim}\\left(\\mathcal{R}(\\mathbf{A})\\right) + {\\rm dim}\\left(\\mathcal{N}(\\mathbf{A})\\right) = n.\\]\n\\({\\rm rank}\\left(\\mathbf{A}^{\\scriptscriptstyle \\top}\\mathbf{A}\\right) = {\\rm rank}\\left(\\mathbf{A}\\mathbf{A}^{\\scriptscriptstyle \\top}\\right) = {\\rm rank}\\left(\\mathbf{A}\\right) = {\\rm rank}\\left(\\mathbf{A}^{\\scriptscriptstyle \\top}\\right)\\)"
  },
  {
    "objectID": "linear-algebra.html#projection-matrix",
    "href": "linear-algebra.html#projection-matrix",
    "title": "Linear Algebra",
    "section": "Projection matrix",
    "text": "Projection matrix\nFor any \\(\\boldsymbol{v}\\in V\\), there are unique vectors \\(\\boldsymbol{x}\\in W\\) and \\(\\boldsymbol{z}\\in W^\\perp\\) such that \\(\\boldsymbol{v} = \\boldsymbol{x}+ \\boldsymbol{z}\\). We call \\(\\boldsymbol{x}\\) the projection of \\(\\boldsymbol{v}\\) onto \\(W\\) and write \\(\\boldsymbol{x}= \\mathbf{P}_W(\\boldsymbol{v})\\). In fact, \\(\\boldsymbol{x}= \\mathbf{P}_W\\boldsymbol{v}\\) where \\(\\mathbf{P}_W\\) is the projection matrix that maps any \\(\\boldsymbol{v}\\in V\\) onto \\(W\\). Similarly \\(\\boldsymbol{z}\\) is the projection of \\(\\boldsymbol{v}\\) onto \\(W^\\perp\\) and the corresponding projection matrix is referred to as the orthogonal projection matrix, \\(\\mathbf{P}_{W^\\perp}\\). Note \\(\\boldsymbol{z} = \\boldsymbol{v} - \\mathbf{P}_W\\boldsymbol{v}=(\\mathbf{I} - \\mathbf{P}_W)\\boldsymbol{v}\\). So \\(\\mathbf{P}_{W^\\perp} = \\mathbf{I} - \\mathbf{P}_W\\).\nThe projection matrix \\(\\mathbf{P}_W\\) has the following basic properties:\n\n\\(\\mathbf{P}_W\\) is idempotent, i.e. \\(\\mathbf{P}_W^2 = \\mathbf{P}_W\\),\n\\(\\mathbf{P}_W\\boldsymbol{x}= \\boldsymbol{x}\\) for all \\(\\boldsymbol{x}\\in W\\) (i.e. \\(\\mathbf{P}_W\\) is the identity operator on \\(W\\)),\nEvery vector \\(\\boldsymbol{x}\\in V\\) may be decomposed uniquely as \\(\\boldsymbol{v} = \\boldsymbol{x}+ \\boldsymbol{z}\\) with \\(\\boldsymbol{x}= \\mathbf{P}_W\\boldsymbol{v}\\) and \\(\\boldsymbol{z} = \\mathbf{P}_{W^\\perp}\\boldsymbol{v} = (\\mathbf{I} - \\mathbf{P}_W)\\boldsymbol{v}\\).\nThere exists matrix \\(\\mathbf{A}\\) with columns corresponding to orthonormal basis of \\(W\\) such that \\(\\mathbf{P}_W = \\mathbf{A}\\mathbf{A}^{\\scriptscriptstyle \\top}\\).\n\n\nProjection onto the range of a matrix\nSuppose \\(W=\\mathcal{R}(\\mathbf{X})\\). Suppose \\(\\mathbf{P}_W\\) is a projection matrix onto \\(W\\) and assume \\(\\mathbf{X}\\) is full-rank, then \\(\\mathbf{P}_W = \\mathbf{X}(\\mathbf{X}^{\\scriptscriptstyle \\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\scriptscriptstyle \\top}\\). Note there are, of course, other projection matrices onto \\(W\\). Note that the \\({\\rm tr}\\left(\\mathbf{P}_W\\right) = {\\rm rank}\\left(\\mathbf{X}\\right)\\)."
  },
  {
    "objectID": "linear-algebra.html#idempotent-matrix",
    "href": "linear-algebra.html#idempotent-matrix",
    "title": "Linear Algebra",
    "section": "Idempotent matrix",
    "text": "Idempotent matrix\nA matrix \\(\\mathbf{A}\\) such that \\(\\mathbf{A}= \\mathbf{A}^2\\) is called idempotent. An idempotent matrix \\(\\mathbf{A}\\) has the following properties:\n\n\\(\\mathbf{A}\\) is a square matrix\nthe eigenvalues of \\(\\mathbf{A}\\) are either 0 or 1\n\\({\\rm tr}\\left(\\mathbf{A}\\right) = {\\rm rank}\\left(\\mathbf{A}\\right)\\)\n\\(\\mathbf{A}\\) is singular unless \\(\\mathbf{A}= \\mathbf{I}\\), the identity matrix\n\\(\\mathbf{I} - \\mathbf{A}\\) is also an idempotent matrix"
  },
  {
    "objectID": "linear-algebra.html#nilpotent-and-unipotent-matrices",
    "href": "linear-algebra.html#nilpotent-and-unipotent-matrices",
    "title": "Linear Algebra",
    "section": "Nilpotent and Unipotent matrices",
    "text": "Nilpotent and Unipotent matrices\n\nA matrix \\(\\mathbf{A}\\) is nilpotent if \\(\\mathbf{A}^2 = \\mathbf{0}\\).\nA matrix \\(\\mathbf{A}\\) is unipotent if \\(\\mathbf{A}^2 = \\mathbf{I}\\)."
  },
  {
    "objectID": "linear-algebra.html#permutation-matrix",
    "href": "linear-algebra.html#permutation-matrix",
    "title": "Linear Algebra",
    "section": "Permutation matrix",
    "text": "Permutation matrix\nA permutation matrix \\(\\mathbf{P}\\) is a square binary matrix that has exactly one entry of 1 in each row and each column and 0s elsewhere. The properties of a permutation matrix include\n\nPre-multiplying by \\(\\mathbf{P}\\) will result in permutation of the rows.\nPost-multiplying by \\(\\mathbf{P}\\) will result in permutation of the columns.\n\\(\\mathbf{P}\\) is a orthogonal matrix.\n\\(\\mathbf{P}^{-1} = \\mathbf{P}^{\\scriptscriptstyle \\top}\\)."
  },
  {
    "objectID": "linear-algebra.html#rotation-matrix",
    "href": "linear-algebra.html#rotation-matrix",
    "title": "Linear Algebra",
    "section": "Rotation matrix",
    "text": "Rotation matrix\nA rotation matrix \\(\\mathbf{R}\\) is a matrix that is used to perform a rotation in Euclidean space. For example the matrix \\[\\mathbf{R} = \\begin{bmatrix}\n\\cos\\theta & -\\sin\\theta\\\\\n\\sin\\theta & \\cos\\theta\n\\end{bmatrix} \\] rotates points in the \\(xy\\)-Cartesian plane (counter)-clockwise through an angle \\(\\theta\\) about the origin of the Cartesian coordinate system by (pre-) post-multiplying."
  },
  {
    "objectID": "linear-algebra.html#vectormatrix-operations",
    "href": "linear-algebra.html#vectormatrix-operations",
    "title": "Linear Algebra",
    "section": "Vector/Matrix operations",
    "text": "Vector/Matrix operations\n\nA scalar product (also called inner product or dot product) for vectors \\(\\boldsymbol{v}\\), \\(\\boldsymbol{w}\\in \\mathbb{R}^n\\) is written \\(\\boldsymbol{v}\\cdot\\boldsymbol{w}\\) and \\[\\boldsymbol{v}\\cdot\\boldsymbol{w} = \\boldsymbol{v}^{\\scriptscriptstyle \\top}\\boldsymbol{w} = \\displaystyle\\sum_{i=1}^nv_i w_i = v_1 w_1 + .. + v_n w_n.\\] The (Euclidean) norm (sometimes called length or magnitude) of a vector \\(\\boldsymbol{v}\\) is written as \\(||\\boldsymbol{v}||\\) and note \\(\\boldsymbol{v}^{\\scriptscriptstyle \\top}\\boldsymbol{v} = \\displaystyle\\sum_{i=1}^n v^2_i=||\\boldsymbol{v}||^2\\).\nA Hadamard product is given as \\(\\mathbf{A}\\cdot \\mathbf{B}= \\{a_{ij}b_{ij}\\}\\).\nA Kronecker product\nA matrix product of \\(\\mathbf{A}= \\begin{bmatrix}\\boldsymbol{a}_1^{\\scriptscriptstyle \\top}\\\\\\boldsymbol{a}_2^{\\scriptscriptstyle \\top}\\\\\\end{bmatrix}\\) and \\(\\mathbf{B}= \\begin{bmatrix}\\boldsymbol{b}_1 & \\boldsymbol{b}_2 & \\boldsymbol{b}_3 & \\boldsymbol{b}_4 \\end{bmatrix}\\): \\[\\mathbf{A}\\mathbf{B}= \\begin{bmatrix}\n\\mathbf{A}\\boldsymbol{b}_1 & \\mathbf{A}\\boldsymbol{b}_2 & \\mathbf{A}\\boldsymbol{b}_3 & \\mathbf{A}\\boldsymbol{b}_4\\end{bmatrix}\\] \\[\\mathbf{A}\\mathbf{B}= \\begin{bmatrix}\n\\boldsymbol{a}_1^{\\scriptscriptstyle \\top}\\boldsymbol{b}_1 & \\boldsymbol{a}_1^{\\scriptscriptstyle \\top}\\boldsymbol{b}_2 & \\boldsymbol{a}_1^{\\scriptscriptstyle \\top}\\boldsymbol{b}_3 & \\boldsymbol{a}_1^{\\scriptscriptstyle \\top}\\boldsymbol{b}_4\\\\\n\\boldsymbol{a}_2^{\\scriptscriptstyle \\top}\\boldsymbol{b}_1 & \\boldsymbol{a}_2^{\\scriptscriptstyle \\top}\\boldsymbol{b}_2 & \\boldsymbol{a}_2^{\\scriptscriptstyle \\top}\\boldsymbol{b}_3 & \\boldsymbol{a}_2^{\\scriptscriptstyle \\top}\\boldsymbol{b}_4\\\\\\end{bmatrix}\\]"
  }
]